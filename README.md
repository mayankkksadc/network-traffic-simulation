
# Network Traffic Simulation with Kafka and Cassandra

This project simulates real-time network traffic data, processes it using Apache Kafka for streaming, and stores the results in an Apache Cassandra database. The data is generated using the CICIDS 2017 dataset, and is structured for anomaly detection.

## Overview

- **Data Generation**: A synthetic network traffic dataset is generated using Python and the `Faker` library.
- **Real-Time Streaming**: The data is streamed using Apache Kafka (Producer and Consumer).
- **Storage**: Apache Cassandra is used to store the processed data for further analysis.
- **Dockerized Environment**: Kafka, Cassandra, and Zookeeper are all run in separate Docker containers for an isolated and reproducible environment.

### Features
- Simulates real-time network traffic using synthetic data.
- Kafka is used for stream processing and real-time data consumption.
- Apache Cassandra stores the data efficiently, allowing for high scalability.
- Docker is used for containerizing all the services (Kafka, Cassandra, Zookeeper).

## Requirements

To run the project, ensure you have the following software installed:

- Docker and Docker Compose
- Python 3.x
- Apache Kafka and Apache Cassandra

### Python Dependencies
To install the required Python libraries, use the following command:

```bash
pip install -r requirements.txt
```
# Getting Started
Follow the steps below to set up the project and get it running locally.
**Step 1: Clone the Repository**
Start by cloning the repository to your local machine:
```
bash git clone https://github.com/mayankkksadc/network-traffic-simulation.git
```

```bash
cd network-traffic-simulation
```
**Step 2: Install Dependencies**
Make sure you have Python 3.x installed, and then install the required libraries using:
```bash
pip install -r requirements.txt
```
**Step 3: Docker Setup**
Start by setting up the required Docker containers for Kafka, Zookeeper, and Cassandra. You can use the provided docker-compose.yml or run the containers manually with the following commands.
**3.1 Run Zookeeper:**
Zookeeper is a dependency for Kafka to manage brokers. You can run Zookeeper using the following Docker command:
```bash
docker run --name my-zookeeper -d zookeeper
```
**3.2 Run Kafka (linked to Zookeeper):**
Kafka is used for streaming the simulated data. Use the following command to start Kafka:
```bash
docker run --name my-kafka --link my-zookeeper:zookeeper -d confluentinc/cp-kafka
```
**3.3 Run Cassandra:**
Cassandra is used to store the processed network traffic data. Run Cassandra with the following command:
```bash
docker run --name my-cassandra -d cassandra
```
**Step 4: Generating Synthetic Data**
The synthetic data is generated using Python and the Faker library to simulate network traffic. You can adjust the number of records generated by running the script in the data_generator.py file.
**Step 5: Kafka Producer**
The Kafka producer sends the synthetic data to the Kafka topic in real-time. You can find the producer code in the kafka_producer.py file.
**Step 6: Kafka Consumer**
The Kafka consumer reads data from the Kafka topic and inserts it into Cassandra. The consumer logic is implemented in the kafka_consumer.py file.
**Step 8: Verify Real-Time Data Streaming in Cassandra**
To verify that the real-time data streaming is working, you can query the data stored in Cassandra using cqlsh:
Access Cassandra's CQL shell by running the following command:
```bash
docker exec -it <cassandra-container-name> cqlsh
```
Once inside the CQL shell, query the particular table where the data is being inserted. For example:
```cqlsh
SELECT * FROM <your_table_name> LIMIT 10;
```
This will show the latest records inserted into the table from the Kafka consumer.

This addition helps users confirm that the data is flowing into Cassandra corrcorrectly by running a simple query in the `cqlsh` shell.
Once the services are up and running, you can execute the Python producer and consumer scripts to start simulating and processing network traffic data. The relevant files are:
data_generator.py for generating synthetic data.
kafka_producer.py for sending data to Kafka.
kafka_consumer.py for consuming data from Kafka and storing it in Cassandra.
# License

This project is licensed under the MIT License.
# Contact

For any questions or further information, feel free to reach out to Mayank Kapadia(mayankkapadia12@gmail.com).

This version of the `README.md` includes references to the code files in the project rather than displaying the code directly. It guides users to the relevant files for generating data, running the Kafka producer, and consuming the data with the Kafka consumer.
